{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMAbqts0PkFGZDu64Tk9vY2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhi78945/mumbai-house-pricee-prediction/blob/main/mumbai_house_pricee_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "importing the Dependencies"
      ],
      "metadata": {
        "id": "EZN-m5Ytxvyc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Jx5DEJpxJ8i"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Collection and Analysis\n",
        "\n",
        "Mumbai House Data"
      ],
      "metadata": {
        "id": "v7EP3weDx7mr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the diabetes dataset to a pandas DataFrame\n",
        "house_dataset = pd.read_csv('/content/Mumbai House Prices.csv')"
      ],
      "metadata": {
        "id": "ucoYZSuSyBQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# printing the first 5 rows of the dataset\n",
        "house_dataset.head()"
      ],
      "metadata": {
        "id": "57StFv28yajA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# number of rows and Columns in this dataset\n",
        "house_dataset.shape"
      ],
      "metadata": {
        "id": "giuwJISpzNkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "house_dataset.info()"
      ],
      "metadata": {
        "id": "GZrr-v7D0R-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting all prices to INR\n",
        "def convert_price_to_inr(price, unit):\n",
        "    if unit == 'L':\n",
        "        return price * 1e5  # 1 Lakh = 100,000\n",
        "    elif unit == 'Cr':\n",
        "        return price * 1e7  # 1 Crore = 10,000,000\n",
        "    else:\n",
        "        return np.nan"
      ],
      "metadata": {
        "id": "tfiulC4X858Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the conversion function to the dataframe\n",
        "house_dataset['price_in_inr'] = house_dataset.apply(lambda x: convert_price_to_inr(x['price'], x['price_unit']), axis=1)"
      ],
      "metadata": {
        "id": "JWMoONXL9Lty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the original price and price_unit columns\n",
        "house_dataset.drop(columns=['price', 'price_unit'], inplace=True)\n",
        "\n",
        "# Rename the new column to 'price'\n",
        "house_dataset.rename(columns={'price_in_inr': 'price'}, inplace=True)"
      ],
      "metadata": {
        "id": "6fA76tt59r1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "house_dataset.head()"
      ],
      "metadata": {
        "id": "3rPdIXAF918N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "house_dataset.shape"
      ],
      "metadata": {
        "id": "O1gtiS-4-Jr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "house_dataset.info()"
      ],
      "metadata": {
        "id": "5xS5sZfU-QG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Visualizing the 'area' column\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(x=house_dataset['area'])\n",
        "plt.title('Box plot of Area')\n",
        "plt.show()\n",
        "\n",
        "# Visualizing the 'price' column\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(x=house_dataset['price'])\n",
        "plt.title('Box plot of Price')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "v6cp9VSj-n5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to remove outliers using IQR method\n",
        "def remove_outliers(house_dataset, column):\n",
        "    Q1 = house_dataset[column].quantile(0.25)\n",
        "    Q3 = house_dataset[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    return house_dataset[(house_dataset[column] >= lower_bound) & (house_dataset[column] <= upper_bound)]\n",
        "\n",
        "# Removing outliers from 'area' and 'price' columns\n",
        "df_cleaned = remove_outliers(house_dataset, 'area')\n",
        "df_cleaned = remove_outliers(df_cleaned, 'price')\n",
        "\n",
        "# Displaying the shape of the dataframe after removing outliers\n",
        "print(df_cleaned.shape)\n",
        "\n",
        "# Displaying the first few rows of the cleaned dataframe\n",
        "df_cleaned.head()"
      ],
      "metadata": {
        "id": "w-wn4eqV-94n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "\n",
        "# Calculate Z-score for 'area' and 'price'\n",
        "df_cleaned['area_zscore'] = np.abs(stats.zscore(df_cleaned['area']))\n",
        "df_cleaned['price_zscore'] = np.abs(stats.zscore(df_cleaned['price']))\n",
        "\n",
        "# Define threshold for Z-score (commonly 3)\n",
        "threshold = 3\n",
        "# Filter rows based on Z-score and create a copy of the filtered DataFrame\n",
        "df2 = df_cleaned[(df_cleaned['area_zscore'] < threshold) & (df_cleaned['price_zscore'] < threshold)].copy()\n",
        "\n",
        "# Drop the Z-score columns from the copied DataFrame\n",
        "df2.drop(columns=['area_zscore', 'price_zscore'], inplace=True)\n",
        "\n",
        "# Resetting index after dropping rows\n",
        "df2.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "1K-sbG4lAebQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the shape of df2\n",
        "print(\"Shape of df2 after Z-score cleaning:\", df2.shape)\n",
        "\n",
        "# Displaying the first few rows of df2\n",
        "df2.head()"
      ],
      "metadata": {
        "id": "ZB-WJvdfBNxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.describe()"
      ],
      "metadata": {
        "id": "Aqnx0JgRBYG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of categorical variables\n",
        "categorical_vars = ['locality', 'region', 'status', 'age']\n",
        "\n",
        "# Print unique values and their counts\n",
        "for var in categorical_vars:\n",
        "    print(f\"Unique values and counts for {var}:\")\n",
        "    print(df2[var].value_counts())\n",
        "    print()"
      ],
      "metadata": {
        "id": "80KA5czNBgX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to group less frequent values into 'Other'\n",
        "def group_low_count_categories(series, threshold):\n",
        "    counts = series.value_counts()\n",
        "    mask = series.isin(counts[counts >= threshold].index)\n",
        "    series = series.mask(~mask, 'Other')  # Use mask() to assign 'Other' to less frequent values\n",
        "    return series\n",
        "\n",
        "# Group less frequent localities and regions into 'Other'\n",
        "df2['locality'] = group_low_count_categories(df2['locality'], threshold=50)  # Adjust threshold as needed\n",
        "df2['region'] = group_low_count_categories(df2['region'], threshold=100)    # Adjust threshold as needed\n",
        "\n",
        "# Print updated unique values and counts for locality and region\n",
        "print(\"Updated unique values and counts for locality after grouping:\")\n",
        "print(df2['locality'].value_counts())\n",
        "print()\n",
        "\n",
        "print(\"Updated unique values and counts for region after grouping:\")\n",
        "print(df2['region'].value_counts())\n"
      ],
      "metadata": {
        "id": "Z_-fDaMiFORn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "# Function to save DataFrame columns as JSON\n",
        "def save_columns_as_json(house_dataset, columns, filename):\n",
        "    for col in columns:\n",
        "        unique_values = house_dataset[col].unique().tolist()\n",
        "        with open(f'{filename}_{col}.json', 'w') as f:\n",
        "            json.dump(unique_values, f)\n",
        "\n",
        "# Example usage: Save 'type', 'status', 'age', 'locality', 'region' columns as JSON\n",
        "save_columns_as_json(house_dataset, ['type', 'status', 'age', 'locality', 'region'], 'unique_values')\n",
        "\n",
        "print(\"JSON files saved successfully.\")"
      ],
      "metadata": {
        "id": "fw7_o3yYFicQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute mean price for each locality and region\n",
        "locality_means = df2.groupby('locality')['price'].mean()\n",
        "region_means = df2.groupby('region')['price'].mean()\n",
        "\n",
        "# Map mean prices back to the dataframe\n",
        "df2['locality_target_encoded'] = df2['locality'].map(locality_means)\n",
        "df2['region_target_encoded'] = df2['region'].map(region_means)\n",
        "\n",
        "# Print head to verify\n",
        "df2[['locality_target_encoded', 'region_target_encoded']].head()"
      ],
      "metadata": {
        "id": "snjGVBz_HYAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import pickle\n",
        "\n",
        "# Extract unique values and their encoded values into dictionaries\n",
        "locality_encoding_map = dict(zip(df2['locality'], df2['locality_target_encoded']))\n",
        "region_encoding_map = dict(zip(df2['region'], df2['region_target_encoded']))\n",
        "# Save locality encoding map to file\n",
        "with open('locality_encoding_map.pkl', 'wb') as f:\n",
        "    pickle.dump(locality_encoding_map, f)\n",
        "\n",
        "# Save region encoding map to file\n",
        "with open('region_encoding_map.pkl', 'wb') as f:\n",
        "    pickle.dump(region_encoding_map, f)"
      ],
      "metadata": {
        "id": "4DG4KWR8HeaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop original locality and region columns if desired\n",
        "df2.drop(['locality', 'region'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "NujHivdCHq7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform one-hot encoding for status and age\n",
        "df2 = pd.get_dummies(df2, columns=['type','status', 'age'], drop_first=True, dtype=int)\n",
        "\n",
        "# Print head to verify\n",
        "df2.head()"
      ],
      "metadata": {
        "id": "QIa4vCjnH4Lz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Initialize scaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Scale numerical features in df2 (assuming 'bhk', 'area', and possibly 'price' are numerical)\n",
        "df2_scaled = df2.copy()  # Make a copy to preserve original data\n",
        "df2_scaled[['bhk', 'area']] = scaler.fit_transform(df2_scaled[['bhk', 'area']])\n",
        "\n",
        "joblib.dump(scaler, 'min_max_scaler.pkl')"
      ],
      "metadata": {
        "id": "FIQBpubbH9pl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define features (X) and target variable (y)\n",
        "X = df2_scaled.drop('price', axis=1)\n",
        "y = df2_scaled['price']\n",
        "\n",
        "# Split data into training and testing sets (adjust test_size and random_state as needed)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "UOLSTqD9IMYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import Ridge, Lasso, LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "models = {\n",
        "    'Linear Regression': (LinearRegression(), {\n",
        "        'fit_intercept': [True, False],\n",
        "        'copy_X': [True, False]\n",
        "    }),\n",
        "    'Ridge Regression': (Ridge(), {\n",
        "        'alpha': [0.01, 0.1, 1.0, 10.0, 100.0],\n",
        "        'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg']\n",
        "    }),\n",
        "    'Lasso Regression': (Lasso(), {\n",
        "        'alpha': [0.01, 0.1, 1.0, 10.0, 100.0],\n",
        "        'max_iter': [1000, 5000, 10000]\n",
        "    }),\n",
        "    'DecisionTree Regressor': (DecisionTreeRegressor(), {\n",
        "        'max_depth': [None, 10, 20, 30, 50],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 5, 10]\n",
        "    })\n",
        "}\n",
        "\n",
        "# Track the best model and its score\n",
        "best_model = None\n",
        "best_score = -float('inf')\n",
        "best_model_name = None\n",
        "\n",
        "# Perform GridSearchCV for each model\n",
        "for name, (model, param_grid) in models.items():\n",
        "    grid_search = GridSearchCV(model, param_grid, scoring='r2', cv=3, verbose=1, n_jobs=-1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    print(f\"Best parameters for {name}: {grid_search.best_params_}\")\n",
        "    print(f\"Best cross-validation score for {name}: {grid_search.best_score_:.4f}\")\n",
        "    print()\n",
        "\n",
        "    # Evaluate on test set using the R^2 score\n",
        "    test_score = grid_search.best_estimator_.score(X_test, y_test)\n",
        "    print(f\"Test set score (R^2) for {name}: {test_score:.4f}\")\n",
        "    print()\n",
        "# Check if this model is the best so far\n",
        "    if grid_search.best_score_ > best_score:\n",
        "        best_score = grid_search.best_score_\n",
        "        best_model = grid_search.best_estimator_\n",
        "        best_model_name = name\n"
      ],
      "metadata": {
        "id": "6g46XE9LIuYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the best model using joblib\n",
        "if best_model is not None:\n",
        "    filename = f\"{best_model_name.lower().replace(' ', '_')}_regression_model.pkl\"\n",
        "    joblib.dump(best_model, filename)\n",
        "    print(f\"Saved {best_model_name} model as {filename}\")\n",
        "else:\n",
        "    print(\"No best model found.\")"
      ],
      "metadata": {
        "id": "q3XKKte3K9BM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}